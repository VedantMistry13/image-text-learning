{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598150903652",
   "display_name": "Python 3.7.7 64-bit ('rnd': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, path\n",
    "from random import sample, choice\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTIONS_FILE_LOC = \"captions.token\"\n",
    "GLOVE_VECTORS_FILE_LOC = \"glove.6B.100d.txt\"\n",
    "SPLIT_FILE_LOC = \"split.pickle\"\n",
    "CAPTIONS_DICT_FILE_LOC = \"captions_dict.pickle\"\n",
    "TOKENIZER_FILE_LOC = \"tokenizer.pickle\"\n",
    "EMBEDDING_MATRIX_FILE_LOC = \"embedding_matrix.pickle\"\n",
    "INCEPTION_FEAT_FILE_LOC = \"inception_v3_output.pickle\"\n",
    "\n",
    "DATASET_DIR = \"./flickr30k-images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "IMAGE_SIZE = 128\n",
    "MAX_WORDS_IN_SENTENCE = 85\n",
    "MARGIN = 0.7\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_OUT_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(caption_file_location=CAPTIONS_FILE_LOC, override=False):\n",
    "    if path.exists(TOKENIZER_FILE_LOC) and not override:\n",
    "        return pickle.load(open(TOKENIZER_FILE_LOC, \"rb\"))\n",
    "    caption_file = open(caption_file_location, \"r\", encoding=\"utf-8\")\n",
    "    captions = []\n",
    "    tokenizer = Tokenizer()\n",
    "    for line in tqdm(caption_file.readlines()):\n",
    "        line = line.split()\n",
    "        caption = [word.lower() for word in line[1:]]\n",
    "        captions.append(caption)\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    pickle.dump(tokenizer, open(TOKENIZER_FILE_LOC, \"wb\"))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = prepare_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_embedding_matrix(embedding_mat_file_location=EMBEDDING_MATRIX_FILE_LOC, override=False):\n",
    "    if path.exists(EMBEDDING_MATRIX_FILE_LOC) and not override:\n",
    "        return pickle.load(open(EMBEDDING_MATRIX_FILE_LOC, \"rb\"))\n",
    "    \n",
    "    embedding_dict = {}\n",
    "    glove_vectors_file = open(GLOVE_VECTORS_FILE_LOC, \"r\", encoding=\"utf-8\")\n",
    "    for line in tqdm(glove_vectors_file.readlines()):\n",
    "        line = line.split()\n",
    "        embedding_dict[line[0]] = np.asarray(line[1:], dtype=np.float64)\n",
    "    \n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_matrix[index, :] = embedding_dict.get(word, embedding_dict[\"unk\"])\n",
    "    pickle.dump(embedding_matrix, open(EMBEDDING_MATRIX_FILE_LOC, \"wb\"))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_matrix = prepare_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_caption_dict(caption_file_location=CAPTIONS_FILE_LOC, override=False):\n",
    "    if path.exists(CAPTIONS_DICT_FILE_LOC) and not override:\n",
    "        return pickle.load(open(CAPTIONS_DICT_FILE_LOC, \"rb\"))\n",
    "    caption_file = open(caption_file_location, \"r\", encoding=\"utf-8\")  \n",
    "    caption_dict = {}\n",
    "    for line in tqdm(caption_file.readlines()):\n",
    "        line = line.split()\n",
    "        head = line[0].split(\".\")\n",
    "        label = head[0]\n",
    "        index = int(head[1].split(\"#\")[1])\n",
    "        tail = [word.lower() for word in line[1:]]\n",
    "        if caption_dict.get(label, None) is None:\n",
    "            caption_dict[label] = {}\n",
    "        caption_dict[label][index] = tail\n",
    "    pickle.dump(caption_dict, open(CAPTIONS_DICT_FILE_LOC, \"wb\"))\n",
    "    return caption_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "caption_dict = prepare_caption_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = pickle.load(open(SPLIT_FILE_LOC, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence_lengths = []\n",
    "for l in caption_dict:\n",
    "    for s in caption_dict[l]:\n",
    "        sentence_lengths.append(len(caption_dict[l][s]))\n",
    "MAX_WORDS_IN_SENTENCE = max(sentence_lengths)\n",
    "print(MAX_WORDS_IN_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception_v3_feats = pickle.load(open(INCEPTION_FEAT_FILE_LOC, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100, input_length=100, weights=[embedding_matrix], trainable=False))\n",
    "# model.add(tf.keras.layers.LSTM(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = tokenizer.texts_to_sequences([[\"the\", \"dog\", \"river\"], [\"two\", \"men\", \"in\", \"a\", \"gray\"]])\n",
    "# padded_docs = pad_sequences(enc, maxlen=100, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import Model\n",
    "\n",
    "# from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# WEIGHTS_FILE = './inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "# inception_v3_model = InceptionV3(\n",
    "#     input_shape = (224, 224, 3), \n",
    "#     include_top = False, \n",
    "#     weights = 'imagenet'\n",
    "# )\n",
    "\n",
    "# # Not required --> inception_v3_model.load_weights(WEIGHTS_FILE)\n",
    "\n",
    "# # Enabling the top 2 inception blocks to train\n",
    "# for layer in model.layers[:249]:\n",
    "#     layer.trainable = False\n",
    "# for layer in model.layers[249:]:\n",
    "#     layer.trainable = True\n",
    "    \n",
    "# # Checking model summary to pick a layer (if required)\n",
    "# inception_v3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_path, image_size=IMAGE_SIZE):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_image(image, channels=3)\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    return image / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return tf.tensordot(a, b, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_ranking_loss(image_pred, text_pred, margin=MARGIN):\n",
    "    n = image_pred.shape[0]\n",
    "\n",
    "    im_loss = 0.0\n",
    "    for i in range(n):\n",
    "        im_pos_sim = tf.keras.backend.sum(tf.multiply(image_pred[i], text_pred[i]))\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            im_neg_sim = tf.keras.backend.sum(tf.multiply(image_pred[i], text_pred[j]))\n",
    "            im_loss += tf.maximum(0, im_neg_sim - im_pos_sim + margin)\n",
    "    \n",
    "    txt_loss = 0.0\n",
    "    for i in range(n):\n",
    "        txt_pos_sim = tf.keras.backend.sum(tf.multiply(image_pred[i], text_pred[i]))\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            txt_neg_sim = tf.keras.backend.sum(tf.multiply(image_pred[j], text_pred[i]))\n",
    "            txt_loss += tf.maximum(0, txt_neg_sim - txt_pos_sim + margin)\n",
    "    \n",
    "    loss = im_loss + txt_loss\n",
    "    return loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_batch(samples, im_size=IMAGE_SIZE):\n",
    "    im_batch = np.zeros((len(samples), im_size, im_size, 3))\n",
    "    for i in range(len(samples)):\n",
    "        im_batch[i] = prepare_image(DATASET_DIR + samples[i] + \".jpg\", im_size)\n",
    "    return im_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_feat_batch(samples):\n",
    "    im_batch = np.zeros((len(samples), 2048))\n",
    "    for i in range(len(samples)):\n",
    "        im_batch[i] = inception_v3_feats[samples[i]]\n",
    "    return im_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_batch(samples):\n",
    "    captions = []\n",
    "    for i in range(len(samples)):\n",
    "        random_index = sample([0, 1, 2, 3, 4], 1)[0]\n",
    "        captions.append(caption_dict[samples[i]][random_index])\n",
    "    encoded_captions = tokenizer.texts_to_sequences(captions)\n",
    "    padded_captions = pad_sequences(encoded_captions, maxlen=MAX_WORDS_IN_SENTENCE, padding=\"post\")\n",
    "    return padded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_batch_one_hot(samples):\n",
    "    captions = []\n",
    "    for i in range(len(samples)):\n",
    "        random_index = sample([0, 1, 2, 3, 4], 1)[0]\n",
    "        captions.append(caption_dict[samples[i]][random_index])\n",
    "    encoded_captions = tokenizer.texts_to_matrix(captions, mode=\"binary\")\n",
    "    return encoded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_caps = prepare_image_batch(sample(splits[\"train\"], 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(padded_caps[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Loading inception v3 network for transfer learning\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras import Model\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "inception_v3_model = InceptionV3(\n",
    "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3), \n",
    "    include_top = False, \n",
    "    weights = 'imagenet'\n",
    ")\n",
    "\n",
    "# Enabling the top 2 inception blocks to train\n",
    "# for layer in inception_v3_model.layers[:249]:\n",
    "#     layer.trainable = False\n",
    "# for layer in inception_v3_model.layers[249:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# Choosing the output layer to be merged with our FC layers (if required)\n",
    "inception_output_layer = inception_v3_model.get_layer('mixed7')\n",
    "# print('Inception model output shape:', inception_output_layer.output_shape)\n",
    "\n",
    "inception_output = inception_v3_model.output\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(inception_output)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dense(EMBEDDING_OUT_LENGTH)(x)\n",
    "x = layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=-1))(x)\n",
    "\n",
    "image_model = Model(inception_v3_model.input, x)\n",
    "# image_model = Sequential()\n",
    "# image_model.add(tf.keras.Input(shape=(2048,)))\n",
    "# image_model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "# image_model.add(layers.Dense(1024, activation=\"relu\"))\n",
    "# image_model.add(layers.Dense(EMBEDDING_OUT_LENGTH))\n",
    "# image_model.add(layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.sum(image_model(prepare_image_batch(sample(splits[\"train\"], 64), 32))[0] ** 2)\n",
    "# # image_model(prepare_image(DATASET_DIR + \"36979\" + \".jpg\", 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_model = Sequential()\n",
    "# text_model.add(layers.Input(vocab_size,))\n",
    "# text_model.add(layers.Dense(512))\n",
    "# text_model.add(layers.Dense(256))\n",
    "# text_model.add(layers.Dense(256))\n",
    "# text_model.add(layers.Dense(EMBEDDING_OUT_LENGTH))\n",
    "# text_model.add(layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1)))\n",
    "text_model.add(layers.Embedding(input_dim=vocab_size, output_dim=100, input_length=MAX_WORDS_IN_SENTENCE, weights=[embedding_matrix], trainable=False))\n",
    "# text_model.add(layers.LSTM(32))\n",
    "# text_model.add(layers.Dense(512, activation=\"relu\"))\n",
    "# text_model.add(layers.Dense(256, activation=\"relu\"))\n",
    "# text_model.add(layers.Dense(EMBEDDING_OUT_LENGTH))\n",
    "# text_model.add(layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1)))\n",
    "\n",
    "# text_model.add(layers.Conv1D(128, 3))\n",
    "# # text_model.add(layers.Bidirectional(layers.LSTM(256, return_sequences=True)))\n",
    "text_model.add(layers.Conv1D(32, 3, padding=\"same\"))\n",
    "text_model.add(layers.GlobalAvgPool1D())\n",
    "text_model.add(layers.Dense(512, activation=\"relu\"))\n",
    "text_model.add(layers.Dense(256, activation=\"relu\"))\n",
    "text_model.add(layers.Dense(EMBEDDING_OUT_LENGTH))\n",
    "text_model.add(layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_model(prepare_text_batch(sample(splits[\"train\"], 64))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(text_model(prepare_text_batch(sample(splits[\"train\"], MAX_WORDS_IN_SENTENCE)))[0] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_gradient(image_input, text_input, image_model=image_model, text_model=text_model, margin=MARGIN):\n",
    "    with tf.GradientTape() as image_tape, tf.GradientTape() as text_tape:\n",
    "        image_tensor = tf.convert_to_tensor(image_input)\n",
    "        text_tensor = tf.convert_to_tensor(np.array(text_input, dtype=np.float32))\n",
    "        image_tape.watch(image_tensor)\n",
    "        text_tape.watch(text_tensor)\n",
    "        image_tensor = image_input\n",
    "        text_tensor = text_input\n",
    "        image_pred = image_model(image_tensor, training=True)\n",
    "        text_pred = text_model(text_tensor, training=True)\n",
    "        loss = triplet_ranking_loss(image_pred, text_pred, margin)\n",
    "        image_grad = image_tape.gradient(loss, image_model.trainable_variables)\n",
    "        text_grad = text_tape.gradient(loss, text_model.trainable_variables)\n",
    "        return loss, image_grad, text_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MARGIN = 0.7\n",
    "ITERATIONS = 2000\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    train_samples = sample(splits[\"train\"], BATCH_SIZE)\n",
    "    train_image_batch = prepare_image_batch(train_samples, IMAGE_SIZE)\n",
    "    train_text_batch = prepare_text_batch(train_samples)\n",
    "    train_loss, image_grad, text_grad = calculate_loss_and_gradient(train_image_batch, train_text_batch, image_model, text_model, MARGIN)\n",
    "    optimizer.apply_gradients(zip(image_grad, image_model.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(text_grad, text_model.trainable_variables))\n",
    "    val_samples = sample(splits[\"val\"], BATCH_SIZE)\n",
    "    val_image_batch = prepare_image_batch(val_samples, IMAGE_SIZE)\n",
    "    val_text_batch = prepare_text_batch(val_samples)\n",
    "    val_image_pred = image_model(val_image_batch)\n",
    "    val_text_pred = text_model(val_text_batch)\n",
    "    val_loss = triplet_ranking_loss(val_image_pred, val_text_pred)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"iteration:\", (i+1), end=\" \")\n",
    "    tf.print(\"train_loss:\", train_loss, \"val_loss:\", val_loss, output_stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.zeros((len(splits[\"test\"]), 2048))\n",
    "for i, label in enumerate(splits[\"test\"]):\n",
    "    test_inputs[i] = inception_v3_feats[label]\n",
    "test_image_outputs = image_model.predict(test_inputs)\n",
    "del test_inputs\n",
    "reference_outs = {}\n",
    "for i, label in enumerate(splits[\"test\"]):\n",
    "    reference_outs[label] = test_image_outputs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.zeros((len(splits[\"test\"]), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, label in enumerate(splits[\"test\"]):\n",
    "    test_inputs[i] = prepare_image(DATASET_DIR + label + \".jpg\", IMAGE_SIZE)\n",
    "test_image_outputs = image_model.predict(test_inputs)\n",
    "del test_inputs\n",
    "reference_outs = {}\n",
    "for i, label in enumerate(splits[\"test\"]):\n",
    "    reference_outs[label] = test_image_outputs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reference_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_query = \"dogs playing with each other .\".split()\n",
    "tokenized = tokenizer.texts_to_sequences([text_query])\n",
    "padded = pad_sequences(tokenized, maxlen=MAX_WORDS_IN_SENTENCE, padding=\"post\")\n",
    "text_output = text_model.predict(padded)\n",
    "print(text_output)\n",
    "sims = []\n",
    "for label in reference_outs:\n",
    "    sims.append([np.sum(text_output * reference_outs[label]), label])\n",
    "sims.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i, out in enumerate(sims):\n",
    "plt.imshow(prepare_image(DATASET_DIR + sims[7][1] + \".jpg\", 512))\n",
    "# if i is 10:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = im_out.shape[0]\n",
    "\n",
    "im_loss = 0.0\n",
    "for i in range(n):\n",
    "    im_pos_sim = cosine_similarity(im_out[i], txt_out[i])\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        im_neg_sim = cosine_similarity(im_out[i], txt_out[j])\n",
    "        im_loss += max(0, im_neg_sim - im_pos_sim + MARGIN)\n",
    "\n",
    "txt_loss = 0.0\n",
    "for i in range(n):\n",
    "    txt_pos_sim = cosine_similarity(im_out[i], txt_out[i])\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            continue\n",
    "        txt_neg_sim = cosine_similarity(im_out[j], txt_out[i])\n",
    "        txt_loss += max(0, txt_neg_sim - txt_pos_sim + MARGIN)\n",
    "\n",
    "loss = im_loss + txt_loss\n",
    "loss / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(im_out[0], txt_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_model(prepare_image_batch(sample(splits[\"train\"], 2), 2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label in tqdm(splits[\"train\"]):\n",
    "    embeddings[label] = image_model(np.array([prepare_image(DATASET_DIR + label + \".jpg\", 256)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in tqdm(splits[\"val\"]):\n",
    "    embeddings[label] = image_model(np.array([prepare_image(DATASET_DIR + label + \".jpg\", 256)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in tqdm(splits[\"test\"]):\n",
    "    embeddings[label] = image_model(np.array([prepare_image(DATASET_DIR + label + \".jpg\", 256)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep_running = True\n",
    "i = 0\n",
    "step = 100\n",
    "train = splits[\"train\"]\n",
    "while keep_running:\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    samples = []\n",
    "    if i + step > len(train):\n",
    "        samples = train[i:]\n",
    "        keep_running = False\n",
    "    else:\n",
    "        samples = train[i:i+step]\n",
    "    batch = prepare_image_batch(samples, len(samples), 256)\n",
    "    out = image_model(batch)\n",
    "    for j in range(len(samples)):\n",
    "        embeddings[samples[j]] = out[j]\n",
    "    i += step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1,2,3][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block_for_image(input_data, filters, conv_size):\n",
    "    x = layers.Conv2D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(filters, conv_size, activation=None, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, input_data])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_net_block_for_text(input_data, filters, conv_size):\n",
    "    x = layers.Conv1D(filters, conv_size, activation='relu', padding='same')(input_data)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv1D(filters, conv_size, activation=None, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, input_data])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image model\n",
    "image_input = tf.keras.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "x = layers.Conv2D(32, 3, activation='relu')(image_input)\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "num_res_net_blocks = 4\n",
    "for i in range(num_res_net_blocks):\n",
    "    x = res_net_block_for_image(x, 64, 3)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(EMBEDDING_OUT_LENGTH)(x)\n",
    "image_output = layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=1))(x)\n",
    "\n",
    "image_model = Model(image_input, image_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.sum(image_model(prepare_image_batch(sample(splits[\"train\"], 64), IMAGE_SIZE))[0] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tf.keras.Input(MAX_WORDS_IN_SENTENCE)\n",
    "y = layers.Embedding(input_dim=vocab_size, output_dim=100, input_length=MAX_WORDS_IN_SENTENCE, weights=[embedding_matrix], trainable=False)(text_input)\n",
    "y = layers.Conv1D(32, 3, activation='relu')(y)\n",
    "y = layers.Conv1D(64, 3, activation='relu')(y)\n",
    "y = layers.MaxPooling1D(3)(y)\n",
    "\n",
    "num_res_net_blocks = 4\n",
    "for i in range(num_res_net_blocks):\n",
    "    y = res_net_block_for_text(y, 64, 3)\n",
    "\n",
    "y = layers.Conv1D(64, 3, activation='relu')(y)\n",
    "y = layers.GlobalAveragePooling1D()(y)\n",
    "y = layers.Dense(256, activation='relu')(y)\n",
    "y = layers.Dropout(0.5)(y)\n",
    "y = layers.Dense(EMBEDDING_OUT_LENGTH)(y)\n",
    "text_output = layers.Lambda(lambda y: tf.keras.backend.l2_normalize(y, axis=1))(y)\n",
    "\n",
    "text_model = Model(text_input, text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(text_model(prepare_text_batch(sample(splits[\"train\"], BATCH_SIZE)))[0] ** 2)\n",
    "# text_model(prepare_text_batch(sample(splits[\"train\"], BATCH_SIZE))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_and_gradient(image_input, text_input, image_model=image_model, text_model=text_model, margin=MARGIN):\n",
    "    with tf.GradientTape() as image_tape, tf.GradientTape() as text_tape:\n",
    "        image_tensor = tf.convert_to_tensor(image_input)\n",
    "        text_tensor = tf.convert_to_tensor(np.array(text_input, dtype=np.float32))\n",
    "        image_tape.watch(image_tensor)\n",
    "        text_tape.watch(text_tensor)\n",
    "        image_tensor = image_input\n",
    "        text_tensor = text_input\n",
    "        image_pred = image_model(image_tensor, training=True)\n",
    "        text_pred = text_model(text_tensor, training=True)\n",
    "        loss = triplet_ranking_loss(image_pred, text_pred, margin)\n",
    "        image_grad = image_tape.gradient(loss, image_model.trainable_variables)\n",
    "        text_grad = text_tape.gradient(loss, text_model.trainable_variables)\n",
    "        return loss, image_grad, text_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "MARGIN = 0.5\n",
    "ITERATIONS = 15000\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 256\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    # train_start_index = (i * BATCH_SIZE) % len(splits[\"train\"])\n",
    "    # train_end_index = (train_start_index + BATCH_SIZE) if train_start_index + BATCH_SIZE < len(splits[\"train\"]) else len(splits[\"train\"])\n",
    "    train_samples = sample(splits[\"train\"], BATCH_SIZE) # [train_start_index:train_end_index]\n",
    "    train_image_batch = prepare_image_batch(train_samples, IMAGE_SIZE)\n",
    "    train_text_batch = prepare_text_batch(train_samples)\n",
    "    train_loss, image_grad, text_grad = calculate_loss_and_gradient(train_image_batch, train_text_batch, image_model, text_model, MARGIN)\n",
    "    optimizer.apply_gradients(zip(image_grad, image_model.trainable_variables))\n",
    "    optimizer.apply_gradients(zip(text_grad, text_model.trainable_variables))\n",
    "    # val_start_index = (i * BATCH_SIZE) % len(splits[\"val\"])\n",
    "    # val_end_index = (val_start_index + BATCH_SIZE) if val_start_index + BATCH_SIZE < len(splits[\"val\"]) else len(splits[\"val\"])\n",
    "    val_samples = sample(splits[\"val\"], BATCH_SIZE) # [val_start_index:val_end_index]\n",
    "    val_image_batch = prepare_image_batch(val_samples, IMAGE_SIZE)\n",
    "    val_text_batch = prepare_text_batch(val_samples)\n",
    "    val_image_pred = image_model(val_image_batch)\n",
    "    val_text_pred = text_model(val_text_batch)\n",
    "    val_loss = triplet_ranking_loss(val_image_pred, val_text_pred)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(\"iteration:\", i+1, \"train_loss:\", train_loss, \"val_loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.save(\"./models/image_model_v1.h5\")\n",
    "text_model.save(\"./models/text_model_v1.h5\")\n",
    "\n",
    "losses = {\n",
    "    \"train\": train_losses,\n",
    "    \"val\": val_losses\n",
    "}\n",
    "pickle.dump(losses, open(\"./losses/losses_v1.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = np.zeros((len(splits[\"test\"]), IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, label in enumerate(splits[\"test\"]):\n",
    "    test_inputs[i] = prepare_image(DATASET_DIR + label + \".jpg\", IMAGE_SIZE)\n",
    "test_image_outputs = image_model.predict(test_inputs)\n",
    "del test_inputs\n",
    "reference_outs = {}\n",
    "for i, label in enumerate(splits[\"test\"]):\n",
    "    reference_outs[label] = test_image_outputs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# text_query = \"people are running .\".split()\n",
    "text_query = caption_dict[choice(splits[\"test\"])][choice([0,1,2,3,4])]\n",
    "tokenized = tokenizer.texts_to_sequences([text_query])\n",
    "padded = pad_sequences(tokenized, maxlen=MAX_WORDS_IN_SENTENCE, padding=\"post\")\n",
    "text_output = text_model.predict(padded)\n",
    "sims = []\n",
    "for label in reference_outs:\n",
    "    sims.append([np.sum(text_output * reference_outs[label]), label])\n",
    "sims.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prepare_image(DATASET_DIR + sims[6][1] + \".jpg\", 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model(prepare_text_batch(sample(splits[\"train\"], MAX_WORDS_IN_SENTENCE)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}